# -*- coding: utf-8 -*-
import os
import mimetypes
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv
from supabase import create_client, Client

# å¾æˆ‘å€‘é‡æ§‹çš„æ¨¡çµ„ä¸­å°å…¥å‡½å¼å’Œé¡
from arxiv_search import search_latest_ai_paper
from podcast_generater import PaperPodcastGenerator, PaperInfo
from config import ARXIV_SEARCH_CONFIG, SUPABASE_CONFIG

def init_supabase_client() -> Client:
    """åˆå§‹åŒ–ä¸¦è¿”å› Supabase å®¢æˆ¶ç«¯"""
    load_dotenv()
    url = os.environ.get("SUPABASE_URL")
    key = os.environ.get("SUPABASE_KEY")
    if not url or not key:
        raise ValueError("Supabase URL å’Œ Key å¿…é ˆåœ¨ .env æ–‡ä»¶ä¸­è¨­å®š")
    return create_client(url, key)

def check_paper_exists(client: Client, arxiv_id: str) -> bool:
    """æª¢æŸ¥è«–æ–‡æ˜¯å¦å·²å­˜åœ¨æ–¼è³‡æ–™åº«ä¸­"""
    try:
        response = client.table("papers").select("id").eq("arxiv_id", arxiv_id).execute()
        return len(response.data) > 0
    except Exception as e:
        print(f"æª¢æŸ¥è«–æ–‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False

def upload_to_storage(client: Client, bucket_name: str, file_path: str, destination_path: str) -> str:
    """ä¸Šå‚³æ–‡ä»¶åˆ° Supabase Storage ä¸¦è¿”å›å…¬é–‹ URL"""
    try:
        content_type, _ = mimetypes.guess_type(file_path)
        options = {"contentType": content_type or "application/octet-stream"}

        with open(file_path, 'rb') as f:
            # é¦–å…ˆå˜—è©¦æ›´æ–°ï¼ˆå¦‚æœæª”æ¡ˆå·²å­˜åœ¨ï¼‰
            try:
                client.storage.from_(bucket_name).update(path=destination_path, file=f, file_options=options)
                print(f"ğŸ”„ æˆåŠŸæ›´æ–° Storage ä¸­çš„æª”æ¡ˆ: {destination_path}")
            except Exception:
                # å¦‚æœæ›´æ–°å¤±æ•—ï¼ˆé€šå¸¸æ˜¯å› ç‚ºæª”æ¡ˆä¸å­˜åœ¨ï¼‰ï¼Œå‰‡ä¸Šå‚³æ–°æª”æ¡ˆ
                f.seek(0) # é‡è¨­æª”æ¡ˆæŒ‡æ¨™
                client.storage.from_(bucket_name).upload(path=destination_path, file=f, file_options=options)
                print(f"ğŸ”¼ æˆåŠŸä¸Šå‚³æ–°æª”æ¡ˆåˆ° Storage: {destination_path}")

        # ç²å–å…¬é–‹ URL
        response = client.storage.from_(bucket_name).get_public_url(destination_path)
        return response
    except Exception as e:
        raise Exception(f"ä¸Šå‚³ {file_path} åˆ° Storage æ™‚å¤±æ•—: {e}")


def insert_paper_to_db(client: Client, paper_data: dict):
    """å°‡è™•ç†å®Œçš„è«–æ–‡è³‡è¨Šæ’å…¥åˆ° Supabase è³‡æ–™åº«ä¸­"""
    try:
        response = client.table("papers").insert(paper_data).execute()
        if len(response.data) == 0:
            raise Exception("æ’å…¥è³‡æ–™å¤±æ•—ï¼Œæ²’æœ‰å›å‚³è³‡æ–™ã€‚")
        print(f"âœ… æˆåŠŸå°‡è«–æ–‡ '{paper_data['title']}' æ’å…¥åˆ°è³‡æ–™åº«")
        return response.data[0]
    except Exception as e:
        raise Exception(f"æ’å…¥è³‡æ–™åº«æ™‚å¤±æ•—: {e}")

def main_workflow():
    """å®Œæ•´çš„å·¥ä½œæµç¨‹"""
    print("ğŸš€ é–‹å§‹åŸ·è¡Œæ¯æ—¥è«–æ–‡æ’­å®¢ç”Ÿæˆå·¥ä½œæµç¨‹...")
    
    try:
        # 1. åˆå§‹åŒ–
        supabase = init_supabase_client()
        podcast_generator = PaperPodcastGenerator()
        
        # 2. æœå°‹æœ€æ–°çš„è«–æ–‡
        print("\nğŸ” æ­£åœ¨å¾ arXiv æœå°‹æœ€æ–°è«–æ–‡...")
        latest_papers = search_latest_ai_paper(
            query=ARXIV_SEARCH_CONFIG["query"],
            max_results=ARXIV_SEARCH_CONFIG["max_results"]
        )
        if not latest_papers:
            print("æ²’æœ‰æ‰¾åˆ°æ–°çš„è«–æ–‡ã€‚å·¥ä½œæµç¨‹çµæŸã€‚")
            return

        for paper in latest_papers:
            arxiv_id = paper['arxiv_id']
            pdf_url = paper['pdf_url']
            print(f"\nğŸ“„ æ‰¾åˆ°è«–æ–‡: {paper['title']} (ID: {arxiv_id})")

            # 3. æª¢æŸ¥è«–æ–‡æ˜¯å¦å·²å­˜åœ¨
            if check_paper_exists(supabase, arxiv_id):
                print(f"âœ… é€™ç¯‡è«–æ–‡ (ID: {arxiv_id}) å·²ç¶“åœ¨è³‡æ–™åº«ä¸­ï¼Œè·³éè™•ç†ã€‚")
                continue

            # 4. è™•ç†æ–°è«–æ–‡
            print(f"âœ¨ æ‰¾åˆ°æ–°è«–æ–‡ï¼Œé–‹å§‹ç”Ÿæˆ Podcast...")
            try:
                # ç”Ÿæˆ Podcast å…§å®¹å’Œæª”æ¡ˆ
                podcast_result = podcast_generator.process_paper(pdf_url=pdf_url)
                paper_info: PaperInfo = podcast_result['paper_info']

                # 5. ä¸Šå‚³éŸ³æª”åˆ° Supabase Storage
                print("â˜ï¸ æ­£åœ¨ä¸Šå‚³éŸ³æª”åˆ° Supabase Storage...")
                bucket_name = SUPABASE_CONFIG["audio"]

                # å°‡éŸ³æª”ä¸Šå‚³åˆ° bucket ä¸­çš„ 'audio' è³‡æ–™å¤¾
                audio_dest_path = f"audio/{arxiv_id}.wav"
                audio_url = upload_to_storage(supabase, bucket_name, podcast_result['audio_path'], audio_dest_path)
                print(f"ğŸ”— éŸ³æª” URL: {audio_url}")

                # 6. æº–å‚™è³‡æ–™ä¸¦å¯«å…¥è³‡æ–™åº«
                # ç¾åœ¨ï¼Œpaper_info çš„å…§å®¹å’Œ script æ–‡å­—ç¨¿ç›´æ¥å­˜å…¥è³‡æ–™åº«ï¼Œä¸å†ä¸Šå‚³å°æ‡‰çš„ .json å’Œ .txt æª”æ¡ˆ
                db_record = {
                    "arxiv_id": arxiv_id,
                    "title": paper_info.title,
                    "authors": paper.get('authors', []),
                    "publish_date": paper.get('updated').isoformat(),
                    "summary": paper_info.abstract,
                    "category": paper_info.field,
                    "innovations": paper_info.innovations,
                    "method": paper_info.method,
                    "results": paper_info.results,
                    "arxiv_url": paper.get('arxiv_url'),
                    "pdf_url": pdf_url,
                    "audio_url": audio_url,
                    "podcast_title": podcast_result['podcast_title'],
                    "podcast_script": podcast_result['script'],
                }
                
                insert_paper_to_db(supabase, db_record)
                print(f"ğŸ‰ æˆåŠŸè™•ç†ä¸¦å„²å­˜è«–æ–‡: {paper_info.title}")

            except Exception as e:
                print(f"è™•ç†è«–æ–‡ {arxiv_id} æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
                # å³ä½¿å–®ç¯‡è«–æ–‡å¤±æ•—ï¼Œä¹Ÿç¹¼çºŒè™•ç†ä¸‹ä¸€ç¯‡
                continue

    except Exception as e:
        print(f"ğŸ˜­ å·¥ä½œæµç¨‹åŸ·è¡Œå¤±æ•—: {e}")

    print("\nğŸ å·¥ä½œæµç¨‹åŸ·è¡Œå®Œç•¢ã€‚")

if __name__ == "__main__":
    main_workflow() 