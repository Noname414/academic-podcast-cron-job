# -*- coding: utf-8 -*-
import logging
import os

# åŒ¯å…¥é‡æ§‹å¾Œçš„æ¨¡çµ„
from config import settings
from logging_config import setup_logging
from arxiv_search import search_latest_ai_paper
from podcast_generater import PaperPodcastGenerator, PaperInfo
from services.supabase_service import SupabaseService
from utils.audio_utils import convert_pcm_to_wav_in_memory
from utils.file_utils import save_output_locally

def process_single_paper(
    paper: dict,
    supabase_service: SupabaseService,
    podcast_generator: PaperPodcastGenerator
):
    """
    è™•ç†å–®ç¯‡æ–°è«–æ–‡çš„å®Œæ•´æµç¨‹ã€‚
    """
    arxiv_id = paper['arxiv_id']
    pdf_url = paper['pdf_url']
    logging.info(f"ğŸ“„ é–‹å§‹è™•ç†æ–°è«–æ–‡: {paper['title']} (ID: {arxiv_id})")

    try:
        # 1. ç”Ÿæˆ Podcast å…§å®¹å’ŒéŸ³è¨Š
        podcast_result = podcast_generator.process_paper(pdf_url=pdf_url)
        paper_info: PaperInfo = podcast_result['paper_info']
        script: str = podcast_result['script']
        audio_data: bytes = podcast_result['audio_data']
        duration: float = podcast_result.get('duration_seconds', 0)

        # 2. å°‡ raw PCM éŸ³è¨Šè½‰æ›ç‚º WAV æ ¼å¼
        wav_data = convert_pcm_to_wav_in_memory(audio_data)

        # 3. (å¯é¸) å„²å­˜æ‰€æœ‰ç”¢å‡ºåˆ°æœ¬åœ°ï¼Œæ–¹ä¾¿é™¤éŒ¯
        # é€éç’°å¢ƒè®Šæ•¸ SAVE_FILES_LOCALLY=true ä¾†å•Ÿç”¨
        if os.getenv("SAVE_FILES_LOCALLY", "false").lower() == "true":
            save_output_locally(
                output_base_folder=settings.OUTPUT_BASE_FOLDER,
                arxiv_id=arxiv_id,
                paper_info=paper_info,
                script=script,
                wav_data=wav_data
            )
            
        # 4. ä¸Šå‚³éŸ³æª”åˆ° Supabase Storage
        logging.info("â˜ï¸ æ­£åœ¨ä¸Šå‚³éŸ³æª”åˆ° Supabase Storage...")
        audio_dest_path = f"{arxiv_id}.wav"
        audio_url = supabase_service.upload_audio(audio_dest_path, wav_data)

        # 5. æº–å‚™è³‡æ–™ä¸¦å¯«å…¥è³‡æ–™åº«
        db_record = {
            "arxiv_id": arxiv_id,
            "title": paper_info.title,
            "authors": paper.get('authors', []),
            "publish_date": paper.get('updated').isoformat(),
            "summary": paper_info.abstract,
            "full_text": script,
            "category": paper.get('category'),
            "tags": paper_info.tags,
            "innovations": paper_info.innovations,
            "method": paper_info.method,
            "results": paper_info.results,
            "arxiv_url": paper.get('arxiv_url'),
            "pdf_url": pdf_url,
            "audio_url": audio_url,
            "duration_seconds": round(duration),
        }
        
        supabase_service.insert_paper(db_record)
        logging.info(f"ğŸ‰ æˆåŠŸè™•ç†ä¸¦å„²å­˜è«–æ–‡: {paper_info.title}")

    except Exception as e:
        logging.error(f"è™•ç†è«–æ–‡ {arxiv_id} æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}", exc_info=True)
        # å³ä½¿å–®ç¯‡è«–æ–‡å¤±æ•—ï¼Œä¹Ÿç¹¼çºŒè™•ç†ä¸‹ä¸€ç¯‡
        pass

def main_workflow():
    """
    é‡æ§‹å¾Œçš„ä¸»å·¥ä½œæµç¨‹ã€‚
    """
    # 1. åˆå§‹åŒ–
    setup_logging()
    logging.info("ğŸš€ é–‹å§‹åŸ·è¡Œæ¯æ—¥è«–æ–‡æ’­å®¢ç”Ÿæˆå·¥ä½œæµç¨‹...")
    
    try:
        # åˆå§‹åŒ–æœå‹™
        supabase_service = SupabaseService(settings)
        podcast_generator = PaperPodcastGenerator(settings.GEMINI_API_KEY)
        
        # 2. æœå°‹æœ€æ–°çš„è«–æ–‡
        logging.info("\nğŸ” æ­£åœ¨å¾ arXiv æœå°‹æœ€æ–°è«–æ–‡...")
        latest_papers = search_latest_ai_paper(
            query=settings.ARXIV_QUERY,
            max_results=settings.ARXIV_MAX_RESULTS
        )
        if not latest_papers:
            logging.info("æ²’æœ‰æ‰¾åˆ°æ–°çš„è«–æ–‡ã€‚å·¥ä½œæµç¨‹çµæŸã€‚")
            return

        new_papers_found = 0
        for paper in latest_papers:
            arxiv_id = paper['arxiv_id']
            
            # 3. æª¢æŸ¥è«–æ–‡æ˜¯å¦å·²å­˜åœ¨
            existing_title = supabase_service.check_paper_exists(arxiv_id)
            if existing_title:
                logging.info(f"âœ… è«–æ–‡ (ID: {arxiv_id}, æ¨™é¡Œ: {existing_title}) å·²å­˜åœ¨ï¼Œè·³éè™•ç†ã€‚")
                continue
            
            new_papers_found += 1
            process_single_paper(paper, supabase_service, podcast_generator)
            
            # é è¨­åªè™•ç†ä¸€ç¯‡æ–°è«–æ–‡ï¼Œä»¥é¿å…è¶…æ™‚
            # è‹¥è¦è™•ç†æ‰€æœ‰æ‰¾åˆ°çš„æ–°è«–æ–‡ï¼Œå¯è¨»è§£æ‰æ­¤è¡Œ
            break
        
        if new_papers_found == 0:
            logging.info("âœ… æ‰€æœ‰æ‰¾åˆ°çš„è«–æ–‡éƒ½å·²è™•ç†éï¼Œæœ¬æ¬¡ç„¡æ–°è«–æ–‡ã€‚")

    except Exception as e:
        logging.critical(f"ğŸ˜­ å·¥ä½œæµç¨‹åŸ·è¡Œå¤±æ•—: {e}", exc_info=True)

    logging.info("\nğŸ å·¥ä½œæµç¨‹åŸ·è¡Œå®Œç•¢ã€‚")

if __name__ == "__main__":
    main_workflow() 